{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning for policyII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from random import sample\n",
    "import math \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset1():\n",
    "    st= [0]*2\n",
    "    return tuple(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_scale=(2365.08,996.88,713.55,1406.84,343.76,3933.12,828.19,2040.95)\n",
    "weibull_shape=(414.16,109.25,79.81,115.21,169.81,143.60,43.83,296.48)\n",
    "tf=(2,6.5,2.5,6,5,3.5,3,3.5)\n",
    "tp=(0.4,5.42,0.625,0.857,1.25,0.7,0.429,0.875)\n",
    "time_interval=5\n",
    "running_time=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal replacement time for tire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step0(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[0],weibull_shape[0])\n",
    "    if action == 0 :\n",
    "        st[0] +=5 #age \n",
    "        \n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "    \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun0(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[0])*tp[0] \n",
    "        \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[0])*time_interval * math.ceil(tf[0]/time_interval)\n",
    "    \n",
    "    if (st[1]==0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=1000\n",
    "min_lr=0.1 \n",
    "min_epsilon=0.1\n",
    "discount=0.9\n",
    "decay=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(epsilon, state):\n",
    "    if (state[1]==1):\n",
    "        return 1\n",
    "    else:\n",
    "        if (np.random.random() < epsilon):\n",
    "            return random.choice([0,1]) \n",
    "        else:\n",
    "            return np.argmax(Q_table[state])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon( t):\n",
    "    \n",
    "    \"\"\"Gets value for epsilon. It declines as we advance in episodes.\"\"\"\n",
    "    # Ensures that there's almost at least a min_epsilon chance of randomly exploring\n",
    "    return max(min_epsilon, min(1., 1. - math.log10((t + 1) / decay)))\n",
    "\n",
    "def get_learning_rate( t):\n",
    "    \n",
    "    \"\"\"Gets value for learning rate. It declines as we advance in episodes.\"\"\"\n",
    "    # Learning rate also declines as we add more episodes\n",
    "    return max(min_lr, min(1., 1. - math.log10((t + 1) / decay)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state)\n",
    "        # Take action\n",
    "        obs = step0(action , list(current_state))\n",
    "        reward = rewardfun0(action,current_state)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_state][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_state]) \n",
    "                                         - Q_table[current_state][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmcXFWd9/HP71bv2TeyQwKEJShKaCEoOCgaAgMGFAdwIWLGuM2IijOCzzOigo7M44g6oygjyOYQkM0oSwyIIsiWgLJKEsKSDZKQrZPuTndV/Z4/zqnu6qTT3enum+p0vu/Xq7ruPecu59Strl+dc0/da+6OiIhImpJSF0BERPo/BRsREUmdgo2IiKROwUZERFKnYCMiIqlTsBERkdQp2IikyMzeZWZLzWyrmZ1R6vKIlIrpdzYi6TGz+4H57v7DUpdFpJTUshFJgZmVxckDgOd6uA2RvZ6CjUgRM3Mz+4KZLTez9Wb2/8wsKcr/pJm9YGYbzWyBmR2ww7qfN7OlwFIzewk4EPhN7EarNLNxZjbfzDaY2TIz+1TR+t8ws1vN7EYz2wJ8Iqb9KqbVmdkzZnaImV1sZmvNbIWZzSjaxvmxfHWxDp8uyjvRzFaa2YVx3TVmdn5RfrWZ/aeZvWpmm83sITOrjnnTzezPZrbJzP5qZiemdAikn1KwEdnZmUAtMA2YBXwSIJ5z+RrwQWAU8Cfgph3WPQM4Fpjq7gcBrwGnu/tAd98el18JjAPOAr5jZicVrT8LuBUYCvwypp0O3AAMA54CFhD+d8cD3wJ+VrT+WuA0YDBwPnCFmU0ryh8DDInrzgF+bGbDYt73gKOBdwLDgX8F8mY2HrgLuCymfwW4zcxGdfI6irRydz300CM+AAdmFs1/Drg/Tt8DzCnKS4B64ICidd+7w/ZeAd4XpycCOWBQUf6/A9fG6W8AD+6w/jeAhUXzpwNbgUycHxT3O3QX9bkTuCBOnwg0AGVF+WuB6bEuDcDb2tnGV4EbdkhbAMwu9fHSY+95qGUjsrMVRdOvElohEM6//DB2JW0CNgBGaCW0t+6OxgEb3L1uh+13tv4bRdMNwHp3zxXNAwwEMLNTzOzR2E23CTgVGFm0/pvuni2ar4/rjgSqgJfa2f8BwIcL9Y7bPR4Y20FdRdpQsBHZ2cSi6f2B1XF6BfBpdx9a9Kh29z8XLd/R8M7VwHAzG7TD9ld1cf0OmVklcBuhO2y0uw8F7iYExM6sBxqBg9rJW0Fo2RTXe4C7f7e7ZZV9j4KNyM7+xcyGmdlE4ALg5pj+U+BiMzsCwMyGmNmHu7pRd18B/Bn4dzOrMrMjCedNftnxml1WAVQC64CsmZ0CzOh4lZay5YFrgO/HQQwZMzsuBrAbgdPN7OSYXhUHG0zopXLLPkDBRmRnvwYWA38hnBi/GsDd7wAuB+bF0WLPAqfs5rbPBSYRWjl3AJe4+8LeKHTsnvsCcAuwEfgIMH83NvEV4BngCUIX4eVAEoPkLMLgiHWEls6/oM8P2Q36UadIETNzYIq7Lyt1WUT6E30zERGR1CnYiIhI6tSNJiIiqVPLRkREUqcL/UUjR470SZMmlboYIiJ7lcWLF693904vXaRgE02aNIlFixaVuhgiInsVM3u1K8upG01ERFKnYCMiIqlTsBERkdQp2IiISOpSCzZmdk28G+CzRWnDzWyhmS2Nz8NiupnZj+KdC58uvtmTmc2Oyy81s9lF6UfHuxYui+taR/sQEZHSSbNlcy0wc4e0iwg3opoC3B/nIVzMcEp8zAWuhBA4gEsIdz48BrikKHhcGZctrDezk32IiEiJpDb02d0fNLNJOyTPItwtEOA64A+EuwDOAq73cDmDR81sqJmNjcsudPcNAGa2EJhpZn8ABrv7IzH9esLteO/pYB+yG9q7soS7g+fBktZngFwzZMohn4MkE9LyOSALlIHnwLaAD4V8E5RVQrYevA6oAasAMmBx3reBN4ANJ9yUMgFvAm8Gqw7z1hy2RTVYFnDIr4NkeNivDYX8WsivApsA5iHd85DfDMlASMoh+zokY4CmsDxDwOohMw7y9ZB7BWxEqJONAGuC/DZwB2sAKwOrBB8cyuprCVf6r4akKpQr+yYkA8JXu3w5kAe2hG2wHWwQUAm5bZDkIL8VMjWQLwt1zWRC3fMJWA4yAyDn4FmwRsg3x/pVg2+J5dkappOR8XWrDOXLNodtm4XXOrc6lM0S8AzYYMitimVMwvGzPOG+alvisW0Kz5mx4PXh2FAf6mE1Yd+F1zozBnJbYz3LQ1mS4ZDfEOqcZMI2ktGQez3uMx8eyZBQ5tyWsH+rDPvxakiawC2+vxohqQl3AsrXQ6aCcEPUMshnQ119W6yLgw0IdfCmWKZtodxJJeQbWsuJhzJ6fdi3lYU8iHUEPAnHO8nFembCcmTia1Afjgv5kJ4kYR9eDklFOEYMiq9LWXhP5LeE40BD2LY3hpfFKuK6SVg3nwUs1M+awSvD/01SE/bl28ArgG1gQ2I9MvH9CVQcGfOGhvImA7Cqd2GW2fkDoRfs6d/ZjHb3NQDuvsbM9ovp42l7h8KVMa2j9JXtpHe0j52Y2VxC64j999+/u3XaY9zz5LJbSDLVQDnuOTyfheZnoGwsZDcANYQ3+urwZmxeBJlh4JvAxkB+E2RXA2shvxFsYHyzDoTmlzBWAtvZ8R5eHd59q8mhIi6xohnWZKE+D9s8PDvwD4ND/sKtsLQZsg5Zx3LggxL4TGyw/rYOXsuGf64EyBgMS+CDcf0nGmBbHioTqDaoNBiUwIT4IZB1KOvKvcKk32oudQH2Ek0730bJ64/GRtyUyu76yo862/t08G6k7xZ3vwq4CqC2trZkF4nL5zbj1ODZV8k1/hGaH4aKv4Pcemj6A+av4F5PYoa7k8NJCj2gFoKQxZcknrqKLRPDdvtz18K28g5rc7A6C6ub4dSBkBjcsgW7sw425Fof2x1/7WAww36wAfvfLW226IMSPAYbu7UOm7+17S7HleEx2Nj/bsEeqG+7/pQKPAYbu3Q99kRj2/y3V+L3hC8LdsoKeHE7DExgQBKep1fj/x6+c9i314cAOCDBB8b8KRVwQk3Y2LPbQ+AclMBAC9tIFLz2SjmH7Q418X9lSRNszIXj3xC/CA3NwPsGhPyfbsRWxS9KDXmod3hLJf6V0LK1c1bB2iw0e2i0Nzu8f0Dre+vI5WFds9YvS2cNxi8NP663ty0PX4SqCo8E/+AgmDMUmhz70hswJIEhGXxIAkMTeHsVHFYZ/h/r8jA4oRv/1F1X84nUNr2ng80bZjY2tjjGAmtj+kra3op3AuHmUitp7RIrpP8hpk9oZ/mO9tFn5HJb8ewqctkXyG+7BnLPtOQlMWh48wNA+OhviYIOjheWaAksxGWSojehdeUN6Q6v57CXmvBpVeGf8pbN2Pc3wqps6J2K8n+dDPuVweZc+IcdkQkf0sMy+PBM+OcrB58zFD99YNjWgARqDKpbTw36FaPxK0aHxleZ7XTW0H8xFs8S/rlyhEdx/n+OxjfloDF+kDSGwNGS/7HB2MpsaP1sjY+hRd0CD9XDK82wNU8S6+cfGIjHYGMfWoltybfd58cH4/8xOuSfviJ8UBQC1aAEf3cNzBwYWlW317XJY0ACozMwJJ2uiX6l2VuP27bYMm5ymF4d8u/bBn/bjm2NedvyUJPgl8UP8wvfgIcbWrZhjY4fUYHfd0DIv+B17C/b2+zSa6vwGGzstjp4rTm0mGtiy7nQYoZwHKsNyi28d8vBD6tozf/I4PC+zBO++rqH/6uCkweE/5PGGOwavPUTeHseFjXAljxszpPEf/r8v44IweaNHMm0l/EqgzFloSxjyvCPDIF314Rtrs7CxPJQvu4Y8l8k1Sd3b90u2NPBZj4wG/hufP51Ufo/mdk8wmCAzTFYLAC+UzQoYAZwsbtvMLM6M5sOPAacB/xXJ/soqez2Z9le9zNo/gOFvm8rtCKKhGCyczpAHifById3MngIKmYG7uTd2wScdi1tIrlxCzy7HXuuCdscPlizd42HaVUwIoO/vQo7vZz8+DIYVwbjymFY/LD81DD8Ux0M8JtaGR67UpPsOg9C91gHqzOlooNMYPbQDpu4hRYQ7vh2Dx9sRa+Z/2QMXtcaqGxrHi/UJ+shgGzNw5vNrcsMTvCZA6EuT3LBGzvtM/8vw+HLI2BNFjvhldYgNCgEJf/kUDhlILyexX60IXQNVhpelYTA9p4aOLQytCIfaWj9Zlz40DuwPATUbfEDp4yQniF0Qw5NwuvaVNSt2fLwEAjLLeTVFeXnPASA8eWhtfdGNnwYN8f0ZsLze2qgKoG/NsLixvCB25DHGsKXAf/6qLD+jZux32wNrYaYR5Pjj04KreKvrsVu2qFVPDDBlx4EgN26Bfv11nCapNBq3b/1I8wnlGPTvKVFmh+YhPdvIf8bo/DGEKBagsngoi8qCzvuSvcfjuk4/6KRHefHLyztGpTBH5scpvMe3oOb8jAgvjerjPwlI7E3suE4vJ6DZ7bDm/Hb2HPbSU5biWcIAXJSORxWgX90SOf/MwBksKr3dmG57kst2JjZTYRWyUgzW0kYVfZd4BYzmwO8BhTu3343cCqwDKgHzgeIQeVSwm1qAb5VGCwAfJYw4q2aMDDgnpi+q33scbnsKrZt/Cre/BLGKozQ+nCInYHhY7E4sHj8216wSTAcJ2MJeW/77dtiF1uLbXlsUSP2eCM81oB/bhj+3hpYn8Ou3wJTK/BZA8kfXgEHlcMh4Q3pJw2AkwbSyVmavZ+1dmW0cdKANrNtAleZ4f87nl0anJB/5ADYGrs8Ct/SDy2ckDX4yJDW9EJQy8W9bMrBnXWxxeYk8RDnR4wOwWZJE8k/rtlpt/mrx4ZuzkcbSD62euf8m8eHb7/3bCX5zOs75981MXzRuLOO5MKdOwLyfzwgvD/urCP5xvqd8xdPgnEJ3L+N5P9taEn3Kgsf6P8yAioyIQhty4e0oZnw+ldbaAlkYgvz8IrWQFJ4Lmzve6Px748O67T3pepLwzvuSz+2uqPcviOx8AWguDU8LAOfGbbr+h1QTv6Ho7GXm+HVJljWDNdthpMHhrG6927FvvMmHFmJH10VjvfUyqJWUA5fNwNG/Q6z8l3tpUd0P5uotrbWe+tCnO55tmz4Evntt7d8ZBstZ0Nazr0Uurp2bMkUlm1PSzdboQXU5hwN2LocmfNeDy2XXBwsc0QF+QuH4ycXRjGxy5Pou2pVyR7m3trlUmGhZVKfh5ebW7sPC+cO3loZujhfz4aWTy52QcZBGLx/QGidvtQE92+LbzArvNHgtIFh/SVN8Eh9a16hhTRjQPjgW9EMy5pCecqLHodUhLS6XOs5kirTua5SK3yJyRg8VI/9zyb4SyO2NrSGvMrwhw4ILdeCsreSjLxtt3ZjZovdvbaz5frKAIF+w72ZDa8fBx6+QSbxo7vQYoG2gQbatiF2K9CszWELtmG/r8cnlePfGAkjM/iYDLxnGPljqvDaqtBdU5Dp/APA2/n+pAC0h1k4J0B50bfbmgSO6KCPcUwZnDlo1/kHVYTHrhxS0dLCbdfE8vDYlUEZ6GD3+6bW//7e216x4vFShemiIFPIO74GP74mdB+vzMKT27FnHMYfSMvoU0tg0Jd7saxtKdj0ss0bv042v4aMhQ/oPE6mJUjs/FYxrE1q2y61nbvT8jjJzXVk5m3FHm/EHHxCGT61IgSxxMhfO5ZCg3XXp3ASQqd+8Rn4CpxBQOHWFPXAcCg7CKs6E8rHgNcQfk+Si78jKQ8P3xh+r5JfS/jtjMVx/UNCzTMVkGuCTDWhs78q/MYiV9h/NvzuxeLr4Q75PEl5Oe758MqYkc/nW89TFdkxgBfSwmtgHS4nsk8ZC7wjTO7J/wQFm16UzTbT0HArhpNzWgJOQYKF32a1031WGGWWjyGpsFYeSDaXkywcTP5D9ZDsR/Lkm1jdRvL/9jH89L/D3n4S5F/FkokkSTn5fI6yTBmwHbOi0TC9pr0ToYVBA5N2vVpZ4XxIUZmSQqurnW/MmfCt3iwpWrz9AQbtBZCupolI+hRsetEb699Lnq1kYlBxQtsB4g+A43ThG3fLXys0fhOM4VjFkWQGXE7yhz+RXH8rdsed0NgI71wERx8NP2+CigraDqY9uGWq9QM5jUAjIrL7FGx6STb7Bs3Z5SRAzpxyjLLYcvHYbsnHsyFtBg3YRCoHfYby6g9gyeDwzfv55+G06fDyyzB0KHzyk3D++TAtXp+0oitDGUVE+g4Fm15Sv/1BsuQpA2owEkvIuQP50BVmrV1mmcxRVNb8A+VVx1NWHsfWP/88rH4C3vc+OPBAOPJI+M534IwzoEotFBHZuynY9JL67U+BDSbxLfH3cHmacMyNjIWuszLbj0EjbqGickrrio8/DpddBr/5DRx2WAg6VVVw550lq4uISG/TzdN6wfbmV9lQfye5fB0Zwkn+BvdwxRXzePWKaobu97vWQLN4McyYAcceCw8/DN/8Jjz4YLrXPRIRKRG1bHrBmo3fIedbqMbJW/gdXTOQ8Ti42Jyagf9IpmxUGNJrBqtXw1//CpdfDp/9LAzSDxREpP9SsOkFFZVvxbY/BGwgF68dGIaYheHM5cnBDH3jQzDnLDj8cLj0UjjttDAAoKamtIUXEdkDFGx6KO/NbGz4HTnfSi4pXKzYwmXPrIL8hkZG/3gMXD01nIs55piwopkCjYjsMxRseqgpu5qG5jeptgpy3kwWyMdf2QxbuJWxX95MUncHfPrT8PWvw+gOrvwqItJPKdj0UFlmGOVUkfd6smY0u+F5J5MYDeMzNB85hqof3wtveUupiyoiUjIKNj30+tY7ac6/QoUZuW15xl5eR2ZLjhXfH07j4RVsnv9lqoYo0IjIvk1Dn3toePUJ5GwAlX/eziEz1rHfdXVkB2fIeoZhA77I6CHpXUVVRGRvoZZND7g7S179NpO+uZLRN26kYVIZz9w8lrpjKqmwGnLUl7qIIiJ9goJND5gZw5sPZPhvN7PqH4ew/MsjyFcnJJ6nmUYGVZ1Q6iKKiPQJCjY9NHjymfz1wdvZNmgzOU9I3CGpYUhVLQMqDi918URE+gSds+mBbc2reXT1uWwbtIWsJzST0ESG5nwTbzYuZs/emkhEpO9SsOmBmrKxjB34YZryCVkSBpQfimNkbRATBs+hIrNfqYsoItInKNj0UHPuZfKW0JgvZ+32V0mS8bjXs7lpaamLJiLSZyjY9MCG7S+yvmk9Ywd+hFy8b2ZF2STcprBfzcm6BbGISKQBAj2Q80Y2Na+nfsvtDK44mJqyibxe/wBmQxhWNb3UxRMR6TPUsumBAWX7UWnbyXvCmsbXeGnrw5gNx30zq7bdU+riiYj0GWrZ9MCA8nFMG/VvbGxawzMbfwHAu8f9hC1NzzFh4MklLp2ISN+hlk0PvFT3IK9se4EXNs1rSfv96gtZ37SF8mRACUsmItK3KNj0wJLNd/Nq3R0kVslZk+/isKFzaMiuZ8mm69jWvLbUxRMR6TPUjdYD7xgxm/u2P8365gZuffUCNjevYWzVkQzIGGVJZamLJyLSZyjY9MDI6sP5h8m/4colJ7O5eTUAs/b/LzJWoWHPIiJF1I3WQ0vq7sfJt8w/uWGeAo2IyA4UbHrgb5t/x31rLmdizdF8+pC7OHzITB5ffx2Pr7+u1EUTEelTShJszOxLZvacmT1rZjeZWZWZTTazx8xsqZndbGYVcdnKOL8s5k8q2s7FMf1FMzu5KH1mTFtmZhelVY91jUuZWDONv59wGeVJNe8d8xUOHzKT9Y3LyHsurd2KiOx19vg5GzMbD3wBmOruDWZ2C3AOcCpwhbvPM7OfAnOAK+PzRnc/2MzOAS4HzjazqXG9I4BxwH1mdkjczY+B9wMrgSfMbL67P9/bdTlq+MeoSqpaBgOYJRw7ci5VmQEklunt3YmI7LVK1Y1WBlSbWRlQA6wB3gvcGvOvA86I07PiPDH/JAsnRWYB89x9u7u/DCwDjomPZe6+3N2bgHlx2V5Vn63jpy9dxL2v34C7A7B++2r+e+mF/Hn9b3p7dyIie7U93rJx91Vm9j3gNaAB+B2wGNjk7tm42EpgfJweD6yI62bNbDMwIqY/WrTp4nVW7JB+bHtlMbO5wFyA/ffff7fqUZ0ZyNTBx/BwDCzHjjiFq5d/nbznOHRw7W5tS0SkvytFN9owQktjMrAJ+BVwSjuLemGVXeTtKr291pq3k4a7XwVcBVBbW9vuMrtiZpwy9hMAPLz+Nzzy5t0MyAxmzkHfYnTV7gUuEZH+rhTdaO8DXnb3de7eDNwOvBMYGrvVACYAq+P0SmAiQMwfAmwoTt9hnV2l9zoz4x3DZ7TMj64+gP0qJ3awhojIvqkUweY1YLqZ1cRzLycBzwMPAGfFZWYDv47T8+M8Mf/3Hk6SzAfOiaPVJgNTgMeBJ4ApcXRbBWEQwfw0KrJ++2quXv51BmQGM2Xg21m+9RnuWn11yzkcEREJSnHO5jEzuxV4EsgCTxG6su4C5pnZZTHt6rjK1cANZraM0KI5J27nuTiS7fm4nc+7h/HGZvZPwAIgA1zj7s/1dj3qs3X8/KV/I+855hz0LfarnMg9a67l4fW/obpsECeNPru3dykistcyfQsPamtrfdGiRbu1zp/W3ckhg6a1nKNxd/6w9jaOHHo8IyrHpFFMEZE+xcwWu3uno6J0bbQeOGHUGW3mzYz3jD5rF0uLiOy7dLmaHli+dXmbeXffKU1ERBRsuu3JjU9y6QuXsuD1BUAINLetuo1LX7iUJXVLSlw6EZG+Rd1o3XTkkCN5x7B3MG9FuEtnXbaOu9bcxYmjTuTggQeXuHQiIn2Lgk03lSVlzD1wLiynJeCcOOpEPn7Ax0lMDUYRkWL6VOyBjGUYUTmiZX501WgFGhGRdqhl002FczT3vn4vJ4w8gYZcAzevuBnDOHnMyZ1vQERkH6Jg002LNy5uOUfz8QM+Tt7z2HJj3op5TB4wmUMGHdL5RkRE9hEKNt00bdg0PjX5U0wfMZ3EEhJLmHvgXI7aeBRTBk4pdfFERPoUBZtuSizhnSPf2SatLCnjuBHHlahEIiJ9l85mi4hI6hRsREQkdQo2IiKSOgUbERFJnYKNiIikTsFGRERSp2AjIiKpU7Dpgbznu5QmIrKvU7Dppttee5QvLLqGxlxTS9r1y//IV5+6keZ8toQlExHpexRsuqmmrJLFG5bzlSevpzHXxPXL/8hPlt5LdaYCw0pdPBGRPkWXq+mmU8YdBcC3nvkVJ953CQAzxryNr7/1w5QlmVIWTUSkz1HLpgdOGXcUBw7cr2X+q0ecqUAjItIOBZseuH75H3lp6xst81996oY253BERCRQsOmmG19+kJ8svZcZY97GQ++/jEve+g8t53C255pLXTwRkT5F52y6aWLNCE4dN42vHfFBypJMyzmcZze9Rrm60kRE2jB3L3UZ+oTa2lpftGhRqYshIrJXMbPF7l7b2XLqRhMRkdQp2IiISOoUbEREJHUKNiIikrqSBBszG2pmt5rZ38zsBTM7zsyGm9lCM1san4fFZc3MfmRmy8zsaTObVrSd2XH5pWY2uyj9aDN7Jq7zIzPT9WNEREqoVC2bHwL3uvthwNuAF4CLgPvdfQpwf5wHOAWYEh9zgSsBzGw4cAlwLHAMcEkhQMVl5hatN3MP1ElERHZhjwcbMxsMvBu4GsDdm9x9EzALuC4udh1wRpyeBVzvwaPAUDMbC5wMLHT3De6+EVgIzIx5g939EQ/juq8v2paIiJRAKVo2BwLrgF+Y2VNm9nMzGwCMdvc1APG5cNGx8cCKovVXxrSO0le2ky4iIiVSimBTBkwDrnT3o4BttHaZtae98y3ejfSdN2w218wWmdmidevWdVxqERHptlIEm5XASnd/LM7fSgg+b8QuMOLz2qLlJxatPwFY3Un6hHbSd+LuV7l7rbvXjho1qkeVEhGRXdvjwcbdXwdWmNmhMekk4HlgPlAYUTYb+HWcng+cF0elTQc2x262BcAMMxsWBwbMABbEvDozmx5HoZ1XtC0RESmBUl2I85+BX5pZBbAcOJ8Q+G4xsznAa8CH47J3A6cCy4D6uCzuvsHMLgWeiMt9y903xOnPAtcC1cA98SEiIiWiC3FGuhCniMju04U4RUSkz1CwERGR1CnYiIhI6hRsREQkdV0ONmZ2vJmdH6dHmdnk9IolIiL9SZeCjZldAnwVuDgmlQM3plUoERHpX7rasjkT+ADh0jK4+2pgUFqFEhGR/qWrwaYpXkHZAeKFM0VERLqkq8HmFjP7GeHy/p8C7gP+J71iiYhIf9Kly9W4+/fM7P3AFuBQ4OvuvjDVkomISL/RabAxswzhApfvI9ygTIBNjQ0MrKikLGltHG5oqGdoVTWJ7kItItJGp91o7p4D6s1syB4oz16hMdvMObffwhcX3EU2nwdgdd0WPvirm/j2Q38obeFERPqgrl71uRF4xswWEkekAbj7F1IpVR9XVVbOmYdN5bsPPwjARe96Nx+781Y2NNRz2pRDO1lbRGTf09Vgc1d8SPTpae8A4LsPP8hdy5ZQXVbGL8/8MEeNGVfikomI9D1dHSBwXbz3zCEx6UV3b06vWHuH06cc2tK6GV5dw1v3G1PiEomI9E1dvYLAicBS4MfAT4AlZvbuFMvV562u28JH7vgVgyoqmHnQFFbVbWlzDkdERFp1tRvtP4EZ7v4igJkdAtwEHJ1WwfqyxmwzH73jV2xoqOe6WR/iqDHj+NmTT/Ddhx9kaFU1l73nfaUuoohIn9LVYFNeCDQA7r7EzMpTKlOfV1VWzmeOPoZDRoxoOUfz6WnvoDKT4djxE0tcOhGRvqerwWaRmV0N3BDnPwosTqdIe4ezj3jrTmmfeNu0EpRERKTv62qw+SzweeALgAEPEs7diIiIdKqrwaYM+KG7fx9aripQmVqpRESkX+nqhTjvB6qL5qsJF+MUERHpVFeDTZW7by3MxOmadIokIiL9TVeDzTYzazn7bWa1QEM6RRIRkf6mq+dsvgj8ysxWE26gNg44O7VSiYhIv9Jhy8bM3mFmY9z9CeAw4GYgC9zJjh5ZAAAOU0lEQVQLvLwHyiciIv1AZ91oPwOa4vRxwNcIl6zZCFyVYrlERKQf6awbLePuG+L02cBV7n4bcJuZ/SXdoomISH/RWcsmY2aFgHQS8PuivK6e7xERkX1cZwHjJuCPZraeMPrsTwBmdjCwOeWyiYhIP9FhsHH3b5vZ/cBY4Hfu7jErAf457cKJiEj/0OnvbNz9UXe/w92Lbwe9xN2f7MmOzSxjZk+Z2W/j/GQze8zMlprZzfFmbZhZZZxfFvMnFW3j4pj+opmdXJQ+M6YtM7OLelJOERHpua7+qDMNFwAvFM1fDlzh7lMIo93mxPQ5wEZ3Pxi4Ii6HmU0FzgGOAGYCP4kBLEMYMXcKMBU4Ny4rIiIlUpJgY2YTgL8Hfh7nDXgvcGtc5DrgjDg9K84T80+Ky88C5rn7dnd/GVgGHBMfy9x9ubs3AfPisiIiUiKlatn8APhXoHAP5RHAJnfPxvmVwPg4PR5YARDzN8flW9J3WGdX6Tsxs7lmtsjMFq1bt66ndRIRkV3Y48HGzE4D1rp78c3XrJ1FvZO83U3fOdH9KnevdffaUaNGdVBqERHpiVL8VuZdwAfM7FSgChhMaOkMNbOy2HqZAKyOy68EJgIr429+hgAbitILitfZVbqIiJTAHm/ZuPvF7j7B3ScRTvD/3t0/CjwAnBUXmw38Ok7Pj/PE/N/HIdjzgXPiaLXJwBTgceAJYEoc3VYR9zF/D1RNRER2oS9dBeCrwDwzuwx4Crg6pl8N3GBmywgtmnMA3P05M7sFeJ5wcdDPu3sOwMz+CVgAZIBr3P25PVoTERFpw1p/p7lvq62t9UWLFpW6GCIiexUzW+zutZ0tV8rf2YiIyD5CwUZERFKnYCMiIqlTsBERkdQp2IiISOoUbEREJHUKNiIikjoFGxERSZ2CjYiIpE7BRkREUqdgIyIiqVOwERGR1CnYiIhI6hRsREQkdQo2IiKSOgUbERFJnYKNiIikTsFGRERSp2AjIiKpU7AREZHUKdiIiEjqFGxERCR1CjYiIpI6BRsREUmdgo2IiKROwUZERFKnYCMiIqlTsBERkdQp2IiISOoUbEREJHUKNiIikro9HmzMbKKZPWBmL5jZc2Z2QUwfbmYLzWxpfB4W083MfmRmy8zsaTObVrSt2XH5pWY2uyj9aDN7Jq7zIzOzPV1PERFpVYqWTRa40N0PB6YDnzezqcBFwP3uPgW4P84DnAJMiY+5wJUQghNwCXAscAxwSSFAxWXmFq03cw/US0REdmGPBxt3X+PuT8bpOuAFYDwwC7guLnYdcEacngVc78GjwFAzGwucDCx09w3uvhFYCMyMeYPd/RF3d+D6om2JiEgJlPScjZlNAo4CHgNGu/saCAEJ2C8uNh5YUbTaypjWUfrKdtLb2/9cM1tkZovWrVvX0+qIiMgulCzYmNlA4Dbgi+6+paNF20nzbqTvnOh+lbvXunvtqFGjOiuyiIh0U0mCjZmVEwLNL9399pj8RuwCIz6vjekrgYlFq08AVneSPqGddBERKZFSjEYz4GrgBXf/flHWfKAwomw28Oui9PPiqLTpwObYzbYAmGFmw+LAgBnAgphXZ2bT477OK9qWiIiUQFkJ9vku4OPAM2b2l5j2NeC7wC1mNgd4DfhwzLsbOBVYBtQD5wO4+wYzuxR4Ii73LXffEKc/C1wLVAP3xIeIiJSIhQFbUltb64sWLSp1MURE9ipmttjdaztbTlcQEBGR1CnYiIhI6hRsREQkdQo2IiKSOgUbERFJnYKNiIikTsFGRERSp2AjIiKpU7AREZHUKdiIiEjqFGxERCR1CjYiIpI6BRsREUmdgo2IiKROwUZERFKnYCMiIqlTsBERkdQp2IiISOoUbEREJHUKNiIikjoFGxERSZ2CjYiIpE7BRkREUqdgIyIiqVOwERGR1CnYiIhI6hRsREQkdQo2IiKSOgUbERFJnYKNiIikrt8GGzObaWYvmtkyM7uoN7ft7tx344Nkm7Mtafl8noU3/JFcLtebuxIR6Rf6ZbAxswzwY+AUYCpwrplN7a3tP/fw37j8vP/i2+f+gGxzlnw+z48+93P+Y/Z/8/Adj/fWbkRE+o2yUhcgJccAy9x9OYCZzQNmAc/3xsbfcvzhfPaKT3Dll67lsnOuYNDQAdz7iwc456IzOeFD03tjFyIi/Up/DTbjgRVF8yuBY3dcyMzmAnMB9t9//93awQcv+HtwuPLL1wLw4QtP55PfPhcz62aRRUT6r37ZjQa094nvOyW4X+Xute5eO2rUqN3aQT6f57W/rWqZX/PyWnJZna8REWlPfw02K4GJRfMTgNW9tfHCOZq7rlrIORedyWev+AQP3f5YyzkcERFpq792oz0BTDGzycAq4BzgI7218ecfWcLd/3Mf51x0Zpuusyu/dC2P/nYxx5+5U4+diMg+zdx36l3qF8zsVOAHQAa4xt2/3dHytbW1vmjRoi5vf8nil5gy7cA252iWLH6JQ44+qJslFhHZ+5jZYnev7Wy5/tqywd3vBu5Oa/vtBRUFGhGR9vXXczYiItKHKNiIiEjqFGxERCR1CjYiIpI6BRsREUldvx36vLvMbB3wajdXHwms78Xi7A1U533Dvlbnfa2+0PM6H+DunV6CRcGmF5jZoq6MM+9PVOd9w75W532tvrDn6qxuNBERSZ2CjYiIpE7BpndcVeoClIDqvG/Y1+q8r9UX9lCddc5GRERSp5aNiIikTsFGRERSp2DTA2Y208xeNLNlZnZRqcvTW8xsopk9YGYvmNlzZnZBTB9uZgvNbGl8HhbTzcx+FF+Hp81sWmlr0H1mljGzp8zst3F+spk9Fut8s5lVxPTKOL8s5k8qZbm7y8yGmtmtZva3eLyP6+/H2cy+FN/Xz5rZTWZW1d+Os5ldY2ZrzezZorTdPq5mNjsuv9TMZvekTAo23WRmGeDHwCnAVOBcM5ta2lL1mixwobsfDkwHPh/rdhFwv7tPAe6P8xBegynxMRe4cs8XuddcALxQNH85cEWs80ZgTkyfA2x094OBK+Jye6MfAve6+2HA2wh177fH2czGA18Aat39LYT7XZ1D/zvO1wIzd0jbreNqZsOBS4BjgWOASwoBqlvcXY9uPIDjgAVF8xcDF5e6XCnV9dfA+4EXgbExbSzwYpz+GXBu0fIty+1ND8Ltw+8H3gv8FjDCL6vLdjzmwALguDhdFpezUtdhN+s7GHh5x3L35+MMjAdWAMPjcfstcHJ/PM7AJODZ7h5X4FzgZ0XpbZbb3YdaNt1XeNMWrIxp/UrsNjgKeAwY7e5rAOLzfnGx/vJa/AD4VyAf50cAm9w9G+eL69VS55i/OS6/NzkQWAf8InYd/tzMBtCPj7O7rwK+B7wGrCEct8X07+NcsLvHtVePt4JN91k7af1qHLmZDQRuA77o7ls6WrSdtL3qtTCz04C17r64OLmdRb0LeXuLMmAacKW7HwVso7VrpT17fZ1jN9AsYDIwDhhA6EbaUX86zp3ZVR17te4KNt23EphYND8BWF2isvQ6MysnBJpfuvvtMfkNMxsb88cCa2N6f3gt3gV8wMxeAeYRutJ+AAw1s8Lt04vr1VLnmD8E2LAnC9wLVgIr3f2xOH8rIfj05+P8PuBld1/n7s3A7cA76d/HuWB3j2uvHm8Fm+57ApgSR7FUEE4yzi9xmXqFmRlwNfCCu3+/KGs+UBiRMptwLqeQfl4c1TId2Fxoru8t3P1id5/g7pMIx/L37v5R4AHgrLjYjnUuvBZnxeX3qm+87v46sMLMDo1JJwHP04+PM6H7bLqZ1cT3eaHO/fY4F9nd47oAmGFmw2KLcEZM655Sn8Tamx/AqcAS4CXg/5S6PL1Yr+MJzeWngb/Ex6mEvur7gaXxeXhc3ggj814CniGM9Cl5PXpQ/xOB38bpA4HHgWXAr4DKmF4V55fF/ANLXe5u1vXtwKJ4rO8EhvX34wx8E/gb8CxwA1DZ344zcBPhnFQzoYUypzvHFfhkrPsy4PyelEmXqxERkdSpG01ERFKnYCMiIqlTsBERkdQp2IiISOoUbEREJHUKNiI9ZGY5M/tL0aPDK4Cb2WfM7Lxe2O8rZjayG+udbGbfiL+fuLun5RDpirLOFxGRTjS4+9u7urC7/zTNwnTBCYQfMb4beLjEZZF9hIKNSEripW9uBt4Tkz7i7svM7BvAVnf/npl9AfgM4bYOz7v7OfHS7tcQfmhYD8x196fNbAThx3qjCD8wtKJ9fYxw6fwKwkVTP+fuuR3Kczbh6uQHEq4PNhrYYmbHuvsH0ngNRArUjSbSc9U7dKOdXZS3xd2PAf6bcK21HV0EHOXuRxKCDoRfuD8V074GXB/TLwEe8nDRzPnA/gBmdjhwNvCu2MLKAR/dcUfufjPh2mfPuvtbCb+gP0qBRvYEtWxEeq6jbrSbip6vaCf/aeCXZnYn4XIxEC4X9CEAd/+9mY0wsyGEbq8PxvS7zGxjXP4k4GjgiXC5L6ppvcjijqYQLksCUOPudV2on0iPKdiIpMt3MV3w94Qg8gHg38zsCDq+tHt72zDgOne/uKOCmNkiYCRQZmbPA2PN7C/AP7v7nzquhkjPqBtNJF1nFz0/UpxhZgkw0d0fINy0bSgwEHiQ2A1mZicC6z3cT6g4/RTCRTMhXFTxLDPbL+YNN7MDdiyIu9cCdxHO1/wH4eKxb1egkT1BLRuRnquOLYSCe929MPy50sweI3yxO3eH9TLAjbGLzIAr3H1THEDwCzN7mjBAoHBZ+G8CN5nZk8AfCZfLx92fN7P/C/wuBrBm4PPAq+2UdRphIMHngO+3ky+SCl31WSQlcTRarbuvL3VZREpN3WgiIpI6tWxERCR1atmIiEjqFGxERCR1CjYiIpI6BRsREUmdgo2IiKTu/wNEM6OArz5DgwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the policy performance\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(scores) + 1)\n",
    "y = scores\n",
    "plt.scatter(x, y, marker='x', c=y)\n",
    "fit = np.polyfit(x, y, deg=4)\n",
    "p = np.poly1d(fit) \n",
    "plt.plot(x,p(x),\"r--\") \n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title(' performance ')\n",
    "plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "760"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[current_state])\n",
    "    if action==1:\n",
    "        optimal.append(current_state)\n",
    "    # Take action\n",
    "    obs = step0(action , list(current_state))\n",
    "    reward = rewardfun0(action,current_state)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal replacement time for transmission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[1],weibull_shape[1])\n",
    "    if action == 0 :\n",
    "        st[0] +=5 #age \n",
    "        \n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "    \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun1(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[1])*tp[1] \n",
    "        \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[1])*time_interval * math.ceil(tf[1]/time_interval)\n",
    "    \n",
    "    if (st[1]==0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state)\n",
    "        # Take action\n",
    "        obs = step1(action , list(current_state))\n",
    "        reward = rewardfun1(action,current_state)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_state][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_state]) \n",
    "                                         - Q_table[current_state][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "845"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[current_state])\n",
    "    if action==1:\n",
    "        optimal.append(current_state)\n",
    "    # Take action\n",
    "    obs = step1(action , list(current_state))\n",
    "    reward = rewardfun1(action,current_state)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal replacement time for wheel rim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step2(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[2],weibull_shape[2])\n",
    "    if action == 0 :\n",
    "        st[0] +=5 #age \n",
    "        \n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "    \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun2(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[2])*tp[2] \n",
    "        \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[2])*time_interval * math.ceil(tf[2]/time_interval)\n",
    "    \n",
    "    if (st[1]==0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state)\n",
    "        # Take action\n",
    "        obs = step2(action , list(current_state))\n",
    "        reward = rewardfun2(action,current_state)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_state][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_state]) \n",
    "                                         - Q_table[current_state][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[current_state])\n",
    "    if action==1:\n",
    "        optimal.append(current_state)\n",
    "    # Take action\n",
    "    obs = step2(action , list(current_state))\n",
    "    reward = rewardfun2(action,current_state)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal replacement time for coupling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step3(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[3],weibull_shape[3])\n",
    "    if action == 0 :\n",
    "        st[0] +=5 #age \n",
    "        \n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "    \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun3(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[3])*tp[3] \n",
    "        \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[3])*time_interval * math.ceil(tf[3]/time_interval)\n",
    "    \n",
    "    if (st[1]==0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state)\n",
    "        # Take action\n",
    "        obs = step3(action , list(current_state))\n",
    "        reward = rewardfun3(action,current_state)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_state][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_state]) \n",
    "                                         - Q_table[current_state][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1315"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[current_state])\n",
    "    if action==1:\n",
    "        optimal.append(current_state)\n",
    "    # Take action\n",
    "    obs = step4(action , list(current_state))\n",
    "    reward = rewardfun4(action,current_state)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal replacement time for motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[4],weibull_shape[4])\n",
    "    if action == 0 :\n",
    "        st[0] +=5 #age \n",
    "        \n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "    \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun4(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[4])*tp[4] \n",
    "        \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[4])*time_interval * math.ceil(tf[4]/time_interval)\n",
    "    \n",
    "    if (st[1]==0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state)\n",
    "        # Take action\n",
    "        obs = step4(action , list(current_state))\n",
    "        reward = rewardfun4(action,current_state)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_state][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_state]) \n",
    "                                         - Q_table[current_state][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[current_state])\n",
    "    if action==1:\n",
    "        optimal.append(current_state)\n",
    "    # Take action\n",
    "    obs = step4(action , list(current_state))\n",
    "    reward = rewardfun4(action,current_state)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal replacement time for brake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step5(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[5],weibull_shape[5])\n",
    "    if action == 0 :\n",
    "        st[0] +=5 #age \n",
    "        \n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "    \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun5(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[5])*tp[5] \n",
    "        \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[5])*time_interval * math.ceil(tf[5]/time_interval)\n",
    "    \n",
    "    if (st[1]==0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state)\n",
    "        # Take action\n",
    "        obs = step5(action , list(current_state))\n",
    "        reward = rewardfun5(action,current_state)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_state][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_state]) \n",
    "                                         - Q_table[current_state][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "745"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[current_state])\n",
    "    if action==1:\n",
    "        optimal.append(current_state)\n",
    "    # Take action\n",
    "    obs = step5(action , list(current_state))\n",
    "    reward = rewardfun5(action,current_state)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal replacement time for steering wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step6(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[6],weibull_shape[6])\n",
    "    if action == 0 :\n",
    "        st[0] +=5 #age \n",
    "        \n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "    \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun6(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[6])*tp[6] \n",
    "        \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[6])*time_interval * math.ceil(tf[6]/time_interval)\n",
    "    \n",
    "    if (st[1]==0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state)\n",
    "        # Take action\n",
    "        obs = step6(action , list(current_state))\n",
    "        reward = rewardfun6(action,current_state)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_state][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_state]) \n",
    "                                         - Q_table[current_state][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[current_state])\n",
    "    if action==1:\n",
    "        optimal.append(current_state)\n",
    "    # Take action\n",
    "    obs = step6(action , list(current_state))\n",
    "    reward = rewardfun6(action,current_state)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal replacement time for shifting gears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step7(action,st): \n",
    "    \n",
    "    f = random.weibullvariate(weibull_scale[7],weibull_shape[7])\n",
    "    if action == 0 :\n",
    "        st[0] +=5 #age \n",
    "        \n",
    "        if f <= st[0]:\n",
    "            st[1]=1    \n",
    "        else:\n",
    "            st[1]=0 \n",
    "    \n",
    "    if action ==1 :\n",
    "            st[0]=0\n",
    "            st[1]=0\n",
    "           \n",
    "    return tuple(st)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardfun7(action,st) :\n",
    "    reward =[]\n",
    "\n",
    "    if action ==1 :\n",
    "        reward= -(time_interval / tp[7])*tp[7] \n",
    "        \n",
    "    if (st[1]==1 and action == 0):\n",
    "        reward= -(time_interval / tp[7])*time_interval * math.ceil(tf[7]/time_interval)\n",
    "    \n",
    "    if (st[1]==0 and action == 0):\n",
    "        reward = 5\n",
    "    return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((100000 ,2) + (2,))\n",
    "scores = []\n",
    "# Looping for each episode\n",
    "for e in range(1000):\n",
    "    # Initializes the state\n",
    "    current_state = reset1()\n",
    "    rewards = []\n",
    "    learning_rate = get_learning_rate(e)\n",
    "    epsilon = 1/(e +1)\n",
    "            \n",
    "            \n",
    "    # Looping for each step\n",
    "    for j in range(running_time//time_interval):\n",
    "        # Choose A from S\n",
    "        action = choose_action(epsilon,current_state)\n",
    "        # Take action\n",
    "        obs = step7(action , list(current_state))\n",
    "        reward = rewardfun7(action,current_state)\n",
    "        rewards.append(reward)\n",
    "        new_state = obs\n",
    "        # Update Q(S,A)\n",
    "        Q_table[current_state][action] += (learning_rate * \n",
    "                                        (reward \n",
    "                                         + discount * np.max(Q_table[new_state]) \n",
    "                                         - Q_table[current_state][action]))\n",
    "        current_state = new_state\n",
    "    \n",
    "    scores.append(sum(rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1055"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal =[]\n",
    "current_state = reset1()\n",
    "\n",
    "for j in range(running_time//time_interval):\n",
    "    # Choose A from S\n",
    "    action = np.argmax(Q_table[current_state])\n",
    "    if action==1:\n",
    "        optimal.append(current_state)\n",
    "    # Take action\n",
    "    obs = step7(action , list(current_state))\n",
    "    reward = rewardfun7(action,current_state)\n",
    "    current_state = obs\n",
    "np.unique(optimal)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
